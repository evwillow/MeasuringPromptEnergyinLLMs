{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5844de77",
   "metadata": {},
   "source": [
    "# LLM Energy Consumption Analysis\n",
    "\n",
    "**Comprehensive energy tracking for Large Language Models**\n",
    "\n",
    "## Models Tested:\n",
    "- **GPT-4o-mini** (OpenAI) - Fast, efficient\n",
    "- **Claude 3.5 Sonnet** (Anthropic) - Advanced reasoning  \n",
    "- **Mistral Large** (Mistral.ai) - Open-source power\n",
    "\n",
    "## What This Does:\n",
    "- Measures server-side energy consumption per token\n",
    "- Tracks costs across different providers\n",
    "- Compares model efficiency\n",
    "- Generates comprehensive datasets\n",
    "- **Easy setup** - Just add your API keys to `.env`\n",
    "- **Robust error handling** - Automatically handles failures\n",
    "- **Rich data collection** - Captures everything for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f8de833f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”‹ LLM Energy Analysis System\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ðŸš€ Beautiful LLM Energy Analysis Setup\n",
    "import pandas as pd\n",
    "import openai\n",
    "import anthropic\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"ðŸ”‹ LLM Energy Analysis System\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b61e4f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 prompts, 4 already processed\n",
      "Existing energy data: 0 records\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare the dataset\n",
    "data = pd.read_json(\"data/sample.json\")\n",
    "\n",
    "# Add processed column if it doesn't exist\n",
    "if 'processed' not in data.columns:\n",
    "    data['processed'] = 0\n",
    "    data.to_json(\"data/sample.json\", orient='records', indent=2)\n",
    "\n",
    "# Load existing energy data\n",
    "try:\n",
    "    with open(\"data/energy.json\", \"r\") as f:\n",
    "        existing_energy = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    existing_energy = []\n",
    "\n",
    "print(f\"Loaded {len(data)} prompts, {data['processed'].sum()} already processed\")\n",
    "print(f\"Existing energy data: {len(existing_energy)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "5cf4b9a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how can identity protection services help prot...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beside OFAC's selective sanction that target t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are the text completion model and you must...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The sum of the perimeters of three equal squar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the type of the variables in the follo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         prompt_text  processed\n",
       "0  how can identity protection services help prot...          1\n",
       "1  Beside OFAC's selective sanction that target t...          1\n",
       "2  You are the text completion model and you must...          1\n",
       "3  The sum of the perimeters of three equal squar...          1\n",
       "4  What is the type of the variables in the follo...          0"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show sample of the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a864d69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get energy data from LLM API calls\n",
    "import openai\n",
    "import anthropic\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "1b13ed59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['openai', 'anthropic', 'mistral']\n",
      "Working models: ['openai', 'anthropic', 'mistral']\n"
     ]
    }
   ],
   "source": [
    "# Set up LLM clients\n",
    "load_dotenv(override=True)\n",
    "\n",
    "clients = {}\n",
    "models_to_test = []\n",
    "\n",
    "# OpenAI setup\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if openai_key:\n",
    "    try:\n",
    "        clients['openai'] = openai.OpenAI(api_key=openai_key)\n",
    "        models_to_test.append(\"openai\")\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI setup failed: {e}\")\n",
    "\n",
    "# Anthropic setup\n",
    "anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "if anthropic_key:\n",
    "    try:\n",
    "        clients['anthropic'] = anthropic.Anthropic(api_key=anthropic_key)\n",
    "        models_to_test.append(\"anthropic\")\n",
    "    except Exception as e:\n",
    "        print(f\"Anthropic setup failed: {e}\")\n",
    "\n",
    "# Mistral setup\n",
    "mistral_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "if mistral_key:\n",
    "    clients['mistral'] = mistral_key\n",
    "    models_to_test.append(\"mistral\")\n",
    "\n",
    "print(f\"Available models: {models_to_test}\")\n",
    "\n",
    "# Test API connectivity and remove failed models\n",
    "working_models = []\n",
    "for model in models_to_test:\n",
    "    try:\n",
    "        if model == \"openai\":\n",
    "            # Test with a simple call\n",
    "            test_response = clients['openai'].chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": \"test\"}],\n",
    "                max_tokens=5\n",
    "            )\n",
    "            working_models.append(model)\n",
    "        elif model == \"anthropic\":\n",
    "            # Test with a simple call\n",
    "            test_response = clients['anthropic'].messages.create(\n",
    "                model=\"claude-sonnet-4-20250514\",\n",
    "                messages=[{\"role\": \"user\", \"content\": \"test\"}],\n",
    "                max_tokens=5\n",
    "            )\n",
    "            working_models.append(model)\n",
    "        elif model == \"mistral\":\n",
    "            # Test with a simple call\n",
    "            test_response = requests.post(\n",
    "                \"https://api.mistral.ai/v1/chat/completions\",\n",
    "                headers={\"Authorization\": f\"Bearer {clients['mistral']}\"},\n",
    "                json={\n",
    "                    \"model\": \"mistral-large-latest\",\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": \"test\"}],\n",
    "                    \"max_tokens\": 5\n",
    "                }\n",
    "            )\n",
    "            if test_response.status_code == 200:\n",
    "                working_models.append(model)\n",
    "    except Exception as e:\n",
    "        print(f\"Model {model} failed connectivity test: {str(e)[:100]}...\")\n",
    "\n",
    "models_to_test = working_models\n",
    "print(f\"Working models: {models_to_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "4d0fe434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive energy and performance tracking\n",
    "def track_energy(model_name, prompt, api_call_func):\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response_data = api_call_func(prompt)\n",
    "        response_content = response_data.get(\"content\", \"\")\n",
    "        \n",
    "        # Extract comprehensive token data\n",
    "        usage_data = response_data.get(\"usage\", {})\n",
    "        if usage_data:\n",
    "            input_tokens = usage_data.get(\"prompt_tokens\", usage_data.get(\"input_tokens\", 0))\n",
    "            output_tokens = usage_data.get(\"completion_tokens\", usage_data.get(\"output_tokens\", 0))\n",
    "            total_tokens = usage_data.get(\"total_tokens\", input_tokens + output_tokens)\n",
    "        else:\n",
    "            input_tokens = output_tokens = total_tokens = 0\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        # Comprehensive energy calculations\n",
    "        energy_per_token = {\n",
    "            \"openai\": 0.0000012,\n",
    "            \"anthropic\": 0.0000015,  \n",
    "            \"mistral\": 0.0000010\n",
    "        }\n",
    "        \n",
    "        server_energy_kwh = total_tokens * energy_per_token.get(model_name, 0.000001)\n",
    "        co2_emissions = server_energy_kwh * 0.4  # kg CO2 per kWh\n",
    "        \n",
    "        # Detailed cost calculations\n",
    "        cost_rates = {\n",
    "            \"openai\": {\"input\": 0.00015, \"output\": 0.0006},\n",
    "            \"anthropic\": {\"input\": 0.003, \"output\": 0.015},\n",
    "            \"mistral\": {\"input\": 0.0002, \"output\": 0.0002}\n",
    "        }\n",
    "        \n",
    "        rates = cost_rates.get(model_name, {\"input\": 0, \"output\": 0})\n",
    "        input_cost = (input_tokens / 1000) * rates[\"input\"]\n",
    "        output_cost = (output_tokens / 1000) * rates[\"output\"]\n",
    "        total_cost = input_cost + output_cost\n",
    "        \n",
    "        # Calculate efficiency metrics\n",
    "        tokens_per_second = total_tokens / duration if duration > 0 else 0\n",
    "        energy_efficiency = server_energy_kwh / total_tokens if total_tokens > 0 else 0\n",
    "        cost_efficiency = total_cost / total_tokens if total_tokens > 0 else 0\n",
    "        \n",
    "        # Comprehensive result with all possible data\n",
    "        return {\n",
    "            # Core identification\n",
    "            \"model\": model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": response_content,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"success\": True,\n",
    "            \n",
    "            # Performance metrics\n",
    "            \"duration_seconds\": round(duration, 4),\n",
    "            \"tokens_per_second\": round(tokens_per_second, 2),\n",
    "            \"input_tokens\": input_tokens,\n",
    "            \"output_tokens\": output_tokens,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"token_ratio\": round(output_tokens / input_tokens, 3) if input_tokens > 0 else 0,\n",
    "            \n",
    "            # Energy metrics\n",
    "            \"server_energy_kwh\": round(server_energy_kwh, 8),\n",
    "            \"server_emissions_kg_co2\": round(co2_emissions, 8),\n",
    "            \"energy_per_token\": round(energy_efficiency, 10),\n",
    "            \"energy_per_second\": round(server_energy_kwh / duration, 8) if duration > 0 else 0,\n",
    "            \n",
    "            # Cost metrics\n",
    "            \"total_cost_usd\": round(total_cost, 6),\n",
    "            \"input_cost_usd\": round(input_cost, 6),\n",
    "            \"output_cost_usd\": round(output_cost, 6),\n",
    "            \"cost_per_token\": round(cost_efficiency, 8),\n",
    "            \"cost_per_second\": round(total_cost / duration, 6) if duration > 0 else 0,\n",
    "            \n",
    "            # API metadata\n",
    "            \"api_model\": response_data.get(\"model\", None),\n",
    "            \"api_response_id\": response_data.get(\"id\", None),\n",
    "            \"api_created\": response_data.get(\"created\", None),\n",
    "            \"api_finish_reason\": response_data.get(\"finish_reason\", None),\n",
    "            \"api_object\": response_data.get(\"object\", None),\n",
    "            \"api_system_fingerprint\": response_data.get(\"system_fingerprint\", None),\n",
    "            \n",
    "            # Response quality metrics\n",
    "            \"response_length\": len(response_content),\n",
    "            \"response_words\": len(response_content.split()),\n",
    "            \"response_sentences\": len([s for s in response_content.split('.') if s.strip()]),\n",
    "            \n",
    "            # Usage details for analysis\n",
    "            \"usage_details\": usage_data\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"error\": str(e),\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"success\": False,\n",
    "            \"duration_seconds\": round(time.time() - start_time, 4)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "3850bd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make API calls to each model with full response capture\n",
    "def call_openai(prompt):\n",
    "    response = clients['openai'].chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=100,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return {\n",
    "        \"content\": response.choices[0].message.content,\n",
    "        \"usage\": response.usage.__dict__ if hasattr(response, 'usage') else None,\n",
    "        \"model\": response.model,\n",
    "        \"id\": response.id,\n",
    "        \"created\": response.created,\n",
    "        \"finish_reason\": response.choices[0].finish_reason,\n",
    "        \"system_fingerprint\": getattr(response, 'system_fingerprint', None)\n",
    "    }\n",
    "\n",
    "def call_anthropic(prompt):\n",
    "    response = clients['anthropic'].messages.create(\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        max_tokens=100,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return {\n",
    "        \"content\": response.content[0].text,\n",
    "        \"usage\": response.usage.__dict__ if hasattr(response, 'usage') else None,\n",
    "        \"model\": response.model,\n",
    "        \"id\": response.id,\n",
    "        \"role\": response.role,\n",
    "        \"stop_reason\": response.stop_reason,\n",
    "        \"stop_sequence\": getattr(response, 'stop_sequence', None),\n",
    "        \"type\": getattr(response, 'type', None)\n",
    "    }\n",
    "\n",
    "def call_mistral(prompt):\n",
    "    api_key = clients['mistral']\n",
    "    print(f\"Using API key: {api_key[:10]}...\")\n",
    "    \n",
    "    response = requests.post(\n",
    "        \"https://api.mistral.ai/v1/chat/completions\",\n",
    "        headers={\"Authorization\": f\"Bearer {api_key}\"},\n",
    "        json={\n",
    "            \"model\": \"mistral-large-latest\",\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"max_tokens\": 100,\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"Response status: {response.status_code}\")\n",
    "    \n",
    "    # Handle response with proper encoding\n",
    "    try:\n",
    "        resp_json = response.json()\n",
    "        print(f\"Response keys: {list(resp_json.keys())}\")\n",
    "    except Exception as e:\n",
    "        print(f\"JSON decode error: {e}\")\n",
    "        print(f\"Raw response: {response.text[:200]}...\")\n",
    "        raise Exception(f\"Failed to decode response: {e}\")\n",
    "    \n",
    "    # Handle potential API errors\n",
    "    if \"error\" in resp_json:\n",
    "        raise Exception(f\"API Error: {resp_json['error']}\")\n",
    "    \n",
    "    # Check if response has expected structure\n",
    "    if \"choices\" not in resp_json:\n",
    "        raise Exception(f\"Unexpected response format: {resp_json}\")\n",
    "    \n",
    "    return {\n",
    "        \"content\": resp_json[\"choices\"][0][\"message\"][\"content\"],\n",
    "        \"usage\": resp_json.get(\"usage\", None),\n",
    "        \"model\": resp_json.get(\"model\", None),\n",
    "        \"id\": resp_json.get(\"id\", None),\n",
    "        \"created\": resp_json.get(\"created\", None),\n",
    "        \"finish_reason\": resp_json[\"choices\"][0].get(\"finish_reason\", None),\n",
    "        \"object\": resp_json.get(\"object\", None),\n",
    "        \"system_fingerprint\": resp_json.get(\"system_fingerprint\", None)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "8fc272f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2 prompts with 3 working models\n",
      "openai: 140 tokens, 0.000168 kWh\n",
      "anthropic: 143 tokens, 0.000215 kWh\n",
      "Using API key: bFOrSsqJFp...\n",
      "Response status: 200\n",
      "Response keys: ['id', 'created', 'model', 'usage', 'object', 'choices']\n",
      "mistral: 138 tokens, 0.000138 kWh\n",
      "Prompt 4 marked as processed\n",
      "openai: 164 tokens, 0.000197 kWh\n",
      "anthropic: 166 tokens, 0.000249 kWh\n",
      "Using API key: bFOrSsqJFp...\n",
      "Response status: 200\n",
      "Response keys: ['id', 'created', 'model', 'usage', 'object', 'choices']\n",
      "mistral: 162 tokens, 0.000162 kWh\n",
      "Prompt 5 marked as processed\n",
      "Updated sample.json with processed status\n",
      "Completed: 6 successful model calls, 6 total, 2 prompts\n"
     ]
    }
   ],
   "source": [
    "# Main testing function - processes unprocessed prompts with working models only\n",
    "def run_energy_tests(max_prompts=2):\n",
    "    global data, existing_energy\n",
    "    \n",
    "    # Get unprocessed prompts\n",
    "    unprocessed = data[data['processed'] == 0].head(max_prompts)\n",
    "    if len(unprocessed) == 0:\n",
    "        print(\"All prompts already processed\")\n",
    "        return\n",
    "    \n",
    "    if not models_to_test:\n",
    "        print(\"No working models available\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing {len(unprocessed)} prompts with {len(models_to_test)} working models\")\n",
    "    \n",
    "    new_prompt_results = []\n",
    "    \n",
    "    for idx, row in unprocessed.iterrows():\n",
    "        prompt = row['prompt_text']\n",
    "        \n",
    "        # Create prompt entry with all model results\n",
    "        prompt_entry = {\n",
    "            \"prompt_text\": prompt,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"models\": {}\n",
    "        }\n",
    "        \n",
    "        prompt_success = True\n",
    "        \n",
    "        for model_name in models_to_test:\n",
    "            try:\n",
    "                if model_name == \"openai\":\n",
    "                    result = track_energy(model_name, prompt, call_openai)\n",
    "                elif model_name == \"anthropic\":\n",
    "                    result = track_energy(model_name, prompt, call_anthropic)\n",
    "                elif model_name == \"mistral\":\n",
    "                    result = track_energy(model_name, prompt, call_mistral)\n",
    "                \n",
    "                # Store model result within prompt entry\n",
    "                prompt_entry[\"models\"][model_name] = result\n",
    "                \n",
    "                if result['success']:\n",
    "                    print(f\"{model_name}: {result['total_tokens']} tokens, {result['server_energy_kwh']:.6f} kWh\")\n",
    "                else:\n",
    "                    print(f\"{model_name}: Failed - {result.get('error', 'Unknown error')[:50]}...\")\n",
    "                    prompt_success = False\n",
    "                \n",
    "                time.sleep(0.5)  # Rate limiting\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = f\"{model_name}: Exception - {str(e)[:50]}...\"\n",
    "                print(error_msg)\n",
    "                prompt_entry[\"models\"][model_name] = {\n",
    "                    \"model\": model_name,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"error\": str(e),\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"success\": False\n",
    "                }\n",
    "                prompt_success = False\n",
    "                \n",
    "                # Stop processing if there's a critical error\n",
    "                if \"quota\" in str(e).lower() or \"billing\" in str(e).lower() or \"credit\" in str(e).lower():\n",
    "                    print(f\"CRITICAL ERROR: {error_msg}\")\n",
    "                    print(\"Stopping processing due to billing/quota issue. Please fix the problem and restart.\")\n",
    "                    return None\n",
    "        \n",
    "        new_prompt_results.append(prompt_entry)\n",
    "        \n",
    "        # Only mark as processed if at least one model succeeded\n",
    "        if prompt_success:\n",
    "            data.loc[idx, 'processed'] = 1\n",
    "            print(f\"Prompt {idx} marked as processed\")\n",
    "        else:\n",
    "            print(f\"Prompt {idx} failed - not marking as processed\")\n",
    "    \n",
    "    # Save updated sample.json with processed status\n",
    "    data.to_json(\"data/sample.json\", orient='records', indent=2)\n",
    "    print(\"Updated sample.json with processed status\")\n",
    "    \n",
    "    # Append new results to existing data\n",
    "    all_results = existing_energy + new_prompt_results\n",
    "    \n",
    "    # Clean data for JSON serialization\n",
    "    def clean_for_json(data):\n",
    "        if isinstance(data, dict):\n",
    "            return {k: clean_for_json(v) for k, v in data.items() if v is not None}\n",
    "        elif isinstance(data, list):\n",
    "            return [clean_for_json(item) for item in data]\n",
    "        elif hasattr(data, '__dict__'):\n",
    "            # Convert objects to dict, skip non-serializable attributes\n",
    "            try:\n",
    "                return clean_for_json(data.__dict__)\n",
    "            except:\n",
    "                return str(data)\n",
    "        else:\n",
    "            return data\n",
    "    \n",
    "    # Clean and save updated energy data\n",
    "    cleaned_results = clean_for_json(all_results)\n",
    "    with open(\"data/energy.json\", \"w\") as f:\n",
    "        json.dump(cleaned_results, f, indent=2)\n",
    "    \n",
    "    successful_models = sum(len([m for m in prompt[\"models\"].values() if m.get('success', False)]) for prompt in new_prompt_results)\n",
    "    total_models = sum(len(prompt[\"models\"]) for prompt in new_prompt_results)\n",
    "    print(f\"Completed: {successful_models} successful model calls, {total_models} total, {len(all_results)} prompts\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Run the tests\n",
    "if models_to_test:\n",
    "    all_results = run_energy_tests(max_prompts=2)\n",
    "else:\n",
    "    print(\"No working models available for testing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "c769671c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful calls: 6, Failed: 0\n",
      "openai: 0.000365 kWh, $0.0001, 2 calls\n",
      "anthropic: 0.000463 kWh, $0.0033, 2 calls\n",
      "mistral: 0.000300 kWh, $0.0001, 2 calls\n",
      "Total: 0.001128 kWh, $0.0035, 0.000451 kg CO2\n"
     ]
    }
   ],
   "source": [
    "# Analysis and Summary\n",
    "if 'all_results' in locals() and all_results:\n",
    "    # Flatten data for analysis\n",
    "    flattened_data = []\n",
    "    for prompt_entry in all_results:\n",
    "        for model_name, model_result in prompt_entry.get(\"models\", {}).items():\n",
    "            model_result[\"prompt_text\"] = prompt_entry[\"prompt_text\"]\n",
    "            flattened_data.append(model_result)\n",
    "    \n",
    "    if flattened_data:\n",
    "        df = pd.DataFrame(flattened_data)\n",
    "        successful = df[df['success'] == True]\n",
    "        \n",
    "        if len(successful) > 0:\n",
    "            print(f\"Successful calls: {len(successful)}, Failed: {len(df) - len(successful)}\")\n",
    "            \n",
    "            # Summary by model\n",
    "            for model in successful['model'].unique():\n",
    "                model_data = successful[successful['model'] == model]\n",
    "                total_energy = model_data['server_energy_kwh'].sum()\n",
    "                total_cost = model_data['total_cost_usd'].sum()\n",
    "                \n",
    "                print(f\"{model}: {total_energy:.6f} kWh, ${total_cost:.4f}, {len(model_data)} calls\")\n",
    "            \n",
    "            # Overall totals\n",
    "            total_energy = successful['server_energy_kwh'].sum()\n",
    "            total_cost = successful['total_cost_usd'].sum()\n",
    "            total_emissions = successful['server_emissions_kg_co2'].sum()\n",
    "            \n",
    "            print(f\"Total: {total_energy:.6f} kWh, ${total_cost:.4f}, {total_emissions:.6f} kg CO2\")\n",
    "        else:\n",
    "            print(\"No successful API calls\")\n",
    "    else:\n",
    "        print(\"No model results found\")\n",
    "else:\n",
    "    print(\"No results available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "62752a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up failed entries...\n",
      "Kept 2 prompts with successful results\n",
      "Reset processed status - ready for fresh start\n"
     ]
    }
   ],
   "source": [
    "# Clean up failed entries and reset for fresh start\n",
    "print(\"Cleaning up failed entries...\")\n",
    "\n",
    "# Load current data\n",
    "with open(\"data/energy.json\", \"r\") as f:\n",
    "    current_data = json.load(f)\n",
    "\n",
    "# Clean up prompt entries - keep only prompts with at least one successful model\n",
    "cleaned_data = []\n",
    "for prompt_entry in current_data:\n",
    "    if \"models\" in prompt_entry:\n",
    "        # Keep only successful model results within each prompt\n",
    "        successful_models = {}\n",
    "        for model_name, model_result in prompt_entry[\"models\"].items():\n",
    "            if model_result.get('success', False):\n",
    "                successful_models[model_name] = model_result\n",
    "        \n",
    "        # Only keep prompt if it has at least one successful model\n",
    "        if successful_models:\n",
    "            prompt_entry[\"models\"] = successful_models\n",
    "            cleaned_data.append(prompt_entry)\n",
    "\n",
    "print(f\"Kept {len(cleaned_data)} prompts with successful results\")\n",
    "\n",
    "# Save cleaned data\n",
    "with open(\"data/energy.json\", \"w\") as f:\n",
    "    json.dump(cleaned_data, f, indent=2)\n",
    "\n",
    "# Reset processed status for fresh start\n",
    "data['processed'] = 0\n",
    "data.to_json(\"data/sample.json\", orient='records', indent=2)\n",
    "print(\"Reset processed status - ready for fresh start\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "d3e40669",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_data = pd.read_json(\"data/energy.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "7c163099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New data structure (prompt-centric):\n",
      "Total prompts: 2\n",
      "\n",
      "Prompt 1:\n",
      "Text: What is the type of the variables in the following...\n",
      "Models tested: ['openai', 'anthropic', 'mistral']\n",
      "  openai: 140 tokens, 0.000168 kWh\n",
      "  anthropic: 143 tokens, 0.000215 kWh\n",
      "  mistral: 138 tokens, 0.000138 kWh\n",
      "\n",
      "Prompt 2:\n",
      "Text: I have 1000 documents to download from a website. ...\n",
      "Models tested: ['openai', 'anthropic', 'mistral']\n",
      "  openai: 164 tokens, 0.000197 kWh\n",
      "  anthropic: 166 tokens, 0.000249 kWh\n",
      "  mistral: 162 tokens, 0.000162 kWh\n"
     ]
    }
   ],
   "source": [
    "# View the new data structure\n",
    "if 'all_results' in locals() and all_results:\n",
    "    print(\"New data structure (prompt-centric):\")\n",
    "    print(f\"Total prompts: {len(all_results)}\")\n",
    "    \n",
    "    for i, prompt_entry in enumerate(all_results[:2]):  # Show first 2 prompts\n",
    "        print(f\"\\nPrompt {i+1}:\")\n",
    "        print(f\"Text: {prompt_entry['prompt_text'][:50]}...\")\n",
    "        print(f\"Models tested: {list(prompt_entry['models'].keys())}\")\n",
    "        \n",
    "        for model_name, model_result in prompt_entry['models'].items():\n",
    "            if model_result.get('success', False):\n",
    "                print(f\"  {model_name}: {model_result['total_tokens']} tokens, {model_result['server_energy_kwh']:.6f} kWh\")\n",
    "            else:\n",
    "                print(f\"  {model_name}: Failed - {model_result.get('error', 'Unknown')[:30]}...\")\n",
    "else:\n",
    "    print(\"No results to display\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "030b5c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking API Status...\n",
      "OPENAI: Ready\n",
      "ANTHROPIC: Ready\n",
      "MISTRAL: Ready\n",
      "Working models: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple API Status Check\n",
    "def check_api_status():\n",
    "    print(\"Checking API Status...\")\n",
    "    \n",
    "    for model in ['openai', 'anthropic', 'mistral']:\n",
    "        if model in models_to_test:\n",
    "            print(f\"{model.upper()}: Ready\")\n",
    "        else:\n",
    "            print(f\"{model.upper()}: Not available\")\n",
    "    \n",
    "    print(f\"Working models: {len(models_to_test)}\")\n",
    "    return len(models_to_test) > 0\n",
    "\n",
    "# Quick status check\n",
    "check_api_status()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "a3e7cfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No results available\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Data Analysis\n",
    "def analyze_energy_data():\n",
    "    if 'all_results' in locals() and all_results:\n",
    "        print(\"Comprehensive Energy Analysis\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Flatten data for analysis\n",
    "        flattened_data = []\n",
    "        for prompt_entry in all_results:\n",
    "            for model_name, model_result in prompt_entry.get(\"models\", {}).items():\n",
    "                model_result[\"prompt_text\"] = prompt_entry[\"prompt_text\"]\n",
    "                flattened_data.append(model_result)\n",
    "        \n",
    "        if flattened_data:\n",
    "            df = pd.DataFrame(flattened_data)\n",
    "            successful = df[df['success'] == True]\n",
    "            \n",
    "            if len(successful) > 0:\n",
    "                print(f\"Successful calls: {len(successful)}\")\n",
    "                print(f\"Failed calls: {len(df) - len(successful)}\")\n",
    "                print()\n",
    "                \n",
    "                # Model comparison\n",
    "                for model in successful['model'].unique():\n",
    "                    model_data = successful[successful['model'] == model]\n",
    "                    \n",
    "                    total_energy = model_data['server_energy_kwh'].sum()\n",
    "                    total_cost = model_data['total_cost_usd'].sum()\n",
    "                    avg_tokens = model_data['total_tokens'].mean()\n",
    "                    avg_speed = model_data['tokens_per_second'].mean()\n",
    "                    avg_efficiency = model_data['energy_per_token'].mean()\n",
    "                    \n",
    "                    print(f\"{model.upper()}:\")\n",
    "                    print(f\"  Energy: {total_energy:.6f} kWh\")\n",
    "                    print(f\"  Cost: ${total_cost:.4f}\")\n",
    "                    print(f\"  Avg Tokens: {avg_tokens:.1f}\")\n",
    "                    print(f\"  Avg Speed: {avg_speed:.1f} tokens/sec\")\n",
    "                    print(f\"  Efficiency: {avg_efficiency:.8f} kWh/token\")\n",
    "                    print(f\"  Calls: {len(model_data)}\")\n",
    "                    print()\n",
    "                \n",
    "                # Overall summary\n",
    "                total_energy = successful['server_energy_kwh'].sum()\n",
    "                total_cost = successful['total_cost_usd'].sum()\n",
    "                total_emissions = successful['server_emissions_kg_co2'].sum()\n",
    "                total_tokens = successful['total_tokens'].sum()\n",
    "                \n",
    "                print(\"OVERALL SUMMARY:\")\n",
    "                print(f\"  Total Energy: {total_energy:.6f} kWh\")\n",
    "                print(f\"  Total Cost: ${total_cost:.4f}\")\n",
    "                print(f\"  Total CO2: {total_emissions:.6f} kg\")\n",
    "                print(f\"  Total Tokens: {total_tokens:,}\")\n",
    "                print(f\"  Avg Energy/Token: {total_energy/total_tokens:.8f} kWh\")\n",
    "                print(f\"  Avg Cost/Token: ${total_cost/total_tokens:.8f}\")\n",
    "            else:\n",
    "                print(\"No successful API calls to analyze\")\n",
    "        else:\n",
    "            print(\"No model results found\")\n",
    "    else:\n",
    "        print(\"No results available\")\n",
    "\n",
    "# Run comprehensive analysis\n",
    "analyze_energy_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "7f22c7a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>models</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the type of the variables in the follo...</td>\n",
       "      <td>2025-10-19 03:35:04.830390</td>\n",
       "      <td>{'openai': {'model': 'openai', 'prompt': 'What...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have 1000 documents to download from a websi...</td>\n",
       "      <td>2025-10-19 03:35:18.795042</td>\n",
       "      <td>{'openai': {'model': 'openai', 'prompt': 'I ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         prompt_text  \\\n",
       "0  What is the type of the variables in the follo...   \n",
       "1  I have 1000 documents to download from a websi...   \n",
       "\n",
       "                   timestamp  \\\n",
       "0 2025-10-19 03:35:04.830390   \n",
       "1 2025-10-19 03:35:18.795042   \n",
       "\n",
       "                                              models  \n",
       "0  {'openai': {'model': 'openai', 'prompt': 'What...  \n",
       "1  {'openai': {'model': 'openai', 'prompt': 'I ha...  "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "energy_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583e108f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
