{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5844de77",
   "metadata": {},
   "source": [
    "# Energy Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8de833f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize dependencies and load environment\n",
    "import pandas as pd, openai, os, json, time, requests\n",
    "from dotenv import load_dotenv\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b61e4f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prompts data and initialize processed status\n",
    "data = pd.read_json(\"data/clean_prompts.jsonl\", lines=True)\n",
    "if 'processed' not in data.columns:\n",
    "    data['processed'] = 0\n",
    "    data.to_json(\"data/clean_prompts.jsonl\", orient='records', lines=True)\n",
    "\n",
    "# Load existing energy data to avoid duplicates\n",
    "try:\n",
    "    existing_energy = pd.read_json(\"data/energy_measurements.jsonl\", lines=True)\n",
    "    processed_combinations = set()\n",
    "    for _, row in existing_energy.iterrows():\n",
    "        processed_combinations.add((row['prompt'], row['model']))\n",
    "except:\n",
    "    processed_combinations = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a864d69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup API clients for different models\n",
    "load_dotenv(override=True)\n",
    "clients, models_to_test = {}, []\n",
    "\n",
    "if openai_key := os.getenv(\"OPENAI_API_KEY\"):\n",
    "    clients['openai'] = openai.OpenAI(api_key=openai_key)\n",
    "    models_to_test.append(\"openai\")\n",
    "\n",
    "if groq_key := os.getenv(\"GROQ_API_KEY\"):\n",
    "    clients['llama'] = groq_key\n",
    "    models_to_test.append(\"llama\")\n",
    "\n",
    "if mistral_key := os.getenv(\"MISTRAL_API_KEY\"):\n",
    "    clients['mistral'] = mistral_key\n",
    "    models_to_test.append(\"mistral\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d0fe434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure energy consumption and performance metrics\n",
    "def track_performance(model_name, prompt, api_call_func):\n",
    "    tracker = EmissionsTracker(log_level=\"ERROR\", save_to_file=False)\n",
    "    tracker.start()\n",
    "    start_time = time.time()\n",
    "    response_data, time_to_first_token = api_call_func(prompt)\n",
    "    duration = time.time() - start_time\n",
    "    energy_consumed_wh = tracker.stop() * 1000\n",
    "    \n",
    "    usage = response_data.get(\"usage\", {})\n",
    "    prompt_tokens = usage.get(\"prompt_tokens\", usage.get(\"input_tokens\", 0))\n",
    "    completion_tokens = usage.get(\"completion_tokens\", usage.get(\"output_tokens\", 0))\n",
    "    total_tokens = usage.get(\"total_tokens\", prompt_tokens + completion_tokens)\n",
    "    content = response_data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"model\": response_data.get(\"model\", model_name),\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"duration\": round(duration, 2),\n",
    "        \"time_to_first_token\": round(time_to_first_token, 3),\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"completion_tokens\": completion_tokens,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"tokens_per_second\": round(total_tokens / duration, 1) if duration > 0 else 0,\n",
    "        \"energy_consumed_wh\": round(energy_consumed_wh, 4),\n",
    "        \"response\": content\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3850bd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API call functions for different models with retry logic\n",
    "import random\n",
    "\n",
    "def api_call_with_retry(api_func, max_retries=3, base_delay=1):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return api_func()\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                print(f\"\\nFinal attempt failed: {str(e)}\")\n",
    "                raise e\n",
    "            \n",
    "            delay = base_delay * (2 ** attempt) + random.uniform(0, 1)\n",
    "            print(f\"\\nAttempt {attempt + 1} failed: {str(e)}. Retrying in {delay:.1f}s...\")\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def call_openai(prompt):\n",
    "    def _call():\n",
    "        start_time = time.time()\n",
    "        response = clients['openai'].chat.completions.create(\n",
    "            model=\"gpt-4o-mini\", messages=[{\"role\": \"user\", \"content\": prompt}], \n",
    "            max_tokens=50, temperature=0.3\n",
    "        )\n",
    "        return {\n",
    "            \"usage\": {\n",
    "                \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "                \"completion_tokens\": response.usage.completion_tokens,\n",
    "                \"total_tokens\": response.usage.total_tokens\n",
    "            },\n",
    "            \"choices\": [{\"message\": {\"content\": choice.message.content}} for choice in response.choices],\n",
    "            \"model\": response.model\n",
    "        }, time.time() - start_time\n",
    "    \n",
    "    return api_call_with_retry(_call)\n",
    "\n",
    "def call_llama(prompt):\n",
    "    def _call():\n",
    "        start_time = time.time()\n",
    "        response = requests.post(\n",
    "            \"https://api.groq.com/openai/v1/chat/completions\",\n",
    "            headers={\"Authorization\": f\"Bearer {clients['llama']}\"},\n",
    "            json={\"model\": \"llama-3.1-8b-instant\", \"messages\": [{\"role\": \"user\", \"content\": prompt}], \n",
    "                  \"max_tokens\": 50, \"temperature\": 0.3},\n",
    "            timeout=30\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"API Error: {response.status_code} - {response.text}\")\n",
    "        return response.json(), time.time() - start_time\n",
    "    \n",
    "    return api_call_with_retry(_call)\n",
    "\n",
    "def call_mistral(prompt):\n",
    "    def _call():\n",
    "        start_time = time.time()\n",
    "        response = requests.post(\n",
    "            \"https://api.mistral.ai/v1/chat/completions\",\n",
    "            headers={\"Authorization\": f\"Bearer {clients['mistral']}\"},\n",
    "            json={\"model\": \"mistral-large-latest\", \"messages\": [{\"role\": \"user\", \"content\": prompt}], \n",
    "                  \"max_tokens\": 50, \"temperature\": 0.3},\n",
    "            timeout=60\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"API Error: {response.status_code} - {response.text}\")\n",
    "        resp_json = response.json()\n",
    "        if \"error\" in resp_json:\n",
    "            raise Exception(f\"API Error: {resp_json['error']}\")\n",
    "        return resp_json, time.time() - start_time\n",
    "    \n",
    "    return api_call_with_retry(_call)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc272f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1473 complete: 3/3 modelsral...Prompt 1490 already complete\n",
      "Prompt 1491 already complete\n",
      "Prompt 1643 complete: 3/3 modelsral...Prompt 1646 already complete\n",
      "Prompt 1859 complete: 3/3 modelsral...Prompt 1870 already complete\n",
      "Prompt 1959 complete: 3/3 modelsral...Prompt 1960 already complete\n",
      "Prompt 2029 complete: 3/3 modelsral...Prompt 2030 already complete\n",
      "Processing prompt 2122 with llama....."
     ]
    }
   ],
   "source": [
    "# Process prompts and save results incrementally\n",
    "def run_performance_tests():\n",
    "    global data, processed_combinations\n",
    "    unprocessed = data[data['processed'] == 0]\n",
    "    if len(unprocessed) == 0:\n",
    "        return\n",
    "    \n",
    "    api_calls = {\"openai\": call_openai, \"llama\": call_llama, \"mistral\": call_mistral}\n",
    "    \n",
    "    for idx, row in unprocessed.iterrows():\n",
    "        prompt_text = row['prompt_text']\n",
    "        prompt_results = []\n",
    "        models_to_process = [model for model in models_to_test if (prompt_text, model) not in processed_combinations]\n",
    "        \n",
    "        if not models_to_process:\n",
    "            print(f\"Prompt {idx + 1} already complete\")\n",
    "            continue\n",
    "            \n",
    "        for model_name in models_to_process:\n",
    "            try:\n",
    "                print(f\"\\rProcessing prompt {idx + 1} with {model_name}...\", end=\"\", flush=True)\n",
    "                result = track_performance(model_name, prompt_text, api_calls[model_name])\n",
    "                prompt_results.append(result)\n",
    "                processed_combinations.add((prompt_text, model_name))\n",
    "            except Exception as e:\n",
    "                print(f\"\\râœ— {model_name} failed: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if prompt_results:\n",
    "            with open(\"data/energy_measurements.jsonl\", \"a\") as f:\n",
    "                for result in prompt_results:\n",
    "                    f.write(json.dumps(result) + \"\\n\")\n",
    "        \n",
    "        print(f\"\\rPrompt {idx + 1} complete: {len(prompt_results)}/{len(models_to_process)} models\", end=\"\", flush=True)\n",
    "        \n",
    "        all_models_complete = all((prompt_text, model) in processed_combinations for model in models_to_test)\n",
    "        if all_models_complete:\n",
    "            data.loc[idx, 'processed'] = 1\n",
    "            data.to_json(\"data/clean_prompts.jsonl\", orient='records', lines=True)\n",
    "\n",
    "if models_to_test:\n",
    "    run_performance_tests()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
