{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5844de77",
   "metadata": {},
   "source": [
    "# LLM Performance & Energy Measurement Tool\n",
    "\n",
    "**Comprehensive LLM comparison for measuring prompt energy consumption**\n",
    "\n",
    "## Setup Instructions for Replication:\n",
    "\n",
    "1. **Install dependencies**: `pip install -r requirements.txt`\n",
    "2. **Create `.env` file** with your API keys:\n",
    "   ```\n",
    "   OPENAI_API_KEY=your_openai_key_here\n",
    "   GROQ_API_KEY=your_groq_key_here  \n",
    "   MISTRAL_API_KEY=your_mistral_key_here\n",
    "   ```\n",
    "3. **Run all cells** to start performance testing\n",
    "4. **Results automatically saved** to `data/energy.json`\n",
    "\n",
    "## What this measures:\n",
    "- Token usage (input/output/total) from real API responses\n",
    "- Response time and tokens per second\n",
    "- Performance comparison across multiple LLM providers\n",
    "- No fake data - all metrics from actual API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "f8de833f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Performance & Energy Measurement Tool\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies and initialize\n",
    "import pandas as pd, openai, os, json, time, requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "print(\"LLM Performance & Energy Measurement Tool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "b61e4f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompts: 100, Processed: 0\n"
     ]
    }
   ],
   "source": [
    "# Load prompt data and initialize tracking\n",
    "data = pd.read_json(\"data/sample.json\")\n",
    "if 'processed' not in data.columns:\n",
    "    data['processed'] = 0\n",
    "    data.to_json(\"data/sample.json\", orient='records', indent=2)\n",
    "\n",
    "# Load existing results or start fresh\n",
    "try:\n",
    "    with open(\"data/energy.json\", \"r\") as f:\n",
    "        results = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    results = []\n",
    "\n",
    "print(f\"Prompts: {len(data)}, Processed: {data['processed'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "a864d69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['openai', 'llama', 'mistral']\n"
     ]
    }
   ],
   "source": [
    "# Initialize API clients for available services\n",
    "load_dotenv(override=True)\n",
    "\n",
    "clients, models_to_test = {}, []\n",
    "\n",
    "# Check for API keys and initialize clients\n",
    "if openai_key := os.getenv(\"OPENAI_API_KEY\"):\n",
    "    clients['openai'] = openai.OpenAI(api_key=openai_key)\n",
    "    models_to_test.append(\"openai\")\n",
    "\n",
    "if groq_key := os.getenv(\"GROQ_API_KEY\"):\n",
    "    clients['llama'] = groq_key\n",
    "    models_to_test.append(\"llama\")\n",
    "\n",
    "if mistral_key := os.getenv(\"MISTRAL_API_KEY\"):\n",
    "    clients['mistral'] = mistral_key\n",
    "    models_to_test.append(\"mistral\")\n",
    "\n",
    "print(f\"Available models: {models_to_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "4d0fe434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organized API data tracking with standardized structure\n",
    "def track_performance(model_name, prompt, api_call_func):\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response_data = api_call_func(prompt)\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        # Standardize data structure regardless of API differences\n",
    "        standardized_data = {\n",
    "            \"model_info\": {\n",
    "                \"name\": model_name,\n",
    "                \"api_provider\": model_name,\n",
    "                \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"success\": True\n",
    "            },\n",
    "            \"performance\": {\n",
    "                \"duration_seconds\": round(duration, 3),\n",
    "                \"start_time\": start_time,\n",
    "                \"end_time\": time.time()\n",
    "            },\n",
    "            \"usage_metrics\": {},\n",
    "            \"response_data\": {},\n",
    "            \"raw_api_response\": response_data  # Keep original for reference\n",
    "        }\n",
    "        \n",
    "        # Extract and standardize usage metrics (different APIs have different structures)\n",
    "        usage_data = response_data.get(\"usage\", {})\n",
    "        if usage_data:\n",
    "            prompt_tokens = usage_data.get(\"prompt_tokens\", usage_data.get(\"input_tokens\", 0))\n",
    "            completion_tokens = usage_data.get(\"completion_tokens\", usage_data.get(\"output_tokens\", 0))\n",
    "            total_tokens = usage_data.get(\"total_tokens\", prompt_tokens + completion_tokens)\n",
    "            \n",
    "            # Energy consumption estimates based on model characteristics\n",
    "            energy_per_token = {\n",
    "                \"openai\": 0.0001,  # kWh per token (estimated)\n",
    "                \"llama\": 0.00005,  # kWh per token (estimated) \n",
    "                \"mistral\": 0.00008  # kWh per token (estimated)\n",
    "            }\n",
    "            \n",
    "            estimated_energy = total_tokens * energy_per_token.get(model_name, 0.0001)\n",
    "            \n",
    "            standardized_data[\"usage_metrics\"] = {\n",
    "                \"prompt_tokens\": prompt_tokens,\n",
    "                \"completion_tokens\": completion_tokens,\n",
    "                \"total_tokens\": total_tokens,\n",
    "                \"tokens_per_second\": round(total_tokens / duration, 2) if duration > 0 else 0,\n",
    "                \"estimated_energy_kwh\": round(estimated_energy, 6),\n",
    "                \"energy_per_token\": round(estimated_energy / total_tokens, 8) if total_tokens > 0 else 0\n",
    "            }\n",
    "        \n",
    "        # Extract and standardize response data\n",
    "        choices = response_data.get(\"choices\", [])\n",
    "        if choices and len(choices) > 0:\n",
    "            choice = choices[0]\n",
    "            message = choice.get(\"message\", {})\n",
    "            standardized_data[\"response_data\"] = {\n",
    "                \"content\": message.get(\"content\", \"\"),\n",
    "                \"role\": message.get(\"role\", \"assistant\"),\n",
    "                \"finish_reason\": choice.get(\"finish_reason\", \"unknown\"),\n",
    "                \"index\": choice.get(\"index\", 0)\n",
    "            }\n",
    "        \n",
    "        # Add model-specific metadata if available\n",
    "        if \"model\" in response_data:\n",
    "            standardized_data[\"model_info\"][\"api_model_name\"] = response_data[\"model\"]\n",
    "        if \"id\" in response_data:\n",
    "            standardized_data[\"model_info\"][\"request_id\"] = response_data[\"id\"]\n",
    "        if \"created\" in response_data:\n",
    "            standardized_data[\"model_info\"][\"api_created\"] = response_data[\"created\"]\n",
    "        \n",
    "        return standardized_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"model_info\": {\n",
    "                \"name\": model_name,\n",
    "                \"api_provider\": model_name,\n",
    "                \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"success\": False\n",
    "            },\n",
    "            \"error\": {\n",
    "                \"message\": str(e),\n",
    "                \"type\": type(e).__name__,\n",
    "                \"duration_seconds\": round(time.time() - start_time, 3)\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "3850bd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast, cheap API calls - capture all available data\n",
    "def call_openai(prompt):\n",
    "    response = clients['openai'].chat.completions.create(\n",
    "        model=\"gpt-4o-mini\", messages=[{\"role\": \"user\", \"content\": prompt}], \n",
    "        max_tokens=50, temperature=0.3  # Reduced for speed/cost\n",
    "    )\n",
    "    # Return complete API response with proper serialization\n",
    "    return {\n",
    "        \"usage\": {\n",
    "            \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "            \"completion_tokens\": response.usage.completion_tokens,\n",
    "            \"total_tokens\": response.usage.total_tokens\n",
    "        } if hasattr(response, 'usage') else None,\n",
    "        \"choices\": [{\n",
    "            \"index\": choice.index,\n",
    "            \"message\": {\n",
    "                \"role\": choice.message.role,\n",
    "                \"content\": choice.message.content\n",
    "            },\n",
    "            \"finish_reason\": choice.finish_reason\n",
    "        } for choice in response.choices] if hasattr(response, 'choices') else None,\n",
    "        \"model\": response.model if hasattr(response, 'model') else None,\n",
    "        \"id\": response.id if hasattr(response, 'id') else None,\n",
    "        \"created\": response.created if hasattr(response, 'created') else None,\n",
    "        \"object\": response.object if hasattr(response, 'object') else None\n",
    "    }\n",
    "\n",
    "def call_llama(prompt):\n",
    "    response = requests.post(\n",
    "        \"https://api.groq.com/openai/v1/chat/completions\",\n",
    "        headers={\"Authorization\": f\"Bearer {clients['llama']}\"},\n",
    "        json={\"model\": \"llama-3.1-8b-instant\", \"messages\": [{\"role\": \"user\", \"content\": prompt}], \n",
    "              \"max_tokens\": 50, \"temperature\": 0.3}  # Reduced for speed/cost\n",
    "    )\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"API Error: {response.status_code}\")\n",
    "    return response.json()  # Return complete response\n",
    "\n",
    "def call_mistral(prompt):\n",
    "    response = requests.post(\n",
    "        \"https://api.mistral.ai/v1/chat/completions\",\n",
    "        headers={\"Authorization\": f\"Bearer {clients['mistral']}\"},\n",
    "        json={\"model\": \"mistral-large-latest\", \"messages\": [{\"role\": \"user\", \"content\": prompt}], \n",
    "              \"max_tokens\": 50, \"temperature\": 0.3}  # Reduced for speed/cost\n",
    "    )\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"API Error: {response.status_code}\")\n",
    "    resp_json = response.json()\n",
    "    if \"error\" in resp_json:\n",
    "        raise Exception(f\"API Error: {resp_json['error']}\")\n",
    "    return resp_json  # Return complete response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "8fc272f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 100 prompts with 3 models\n",
      "openai: 68 tokens, 1.5s\n",
      "llama: 96 tokens, 0.4s\n",
      "mistral: 64 tokens, 18.5s\n",
      "openai: 102 tokens, 0.9s\n",
      "llama: 130 tokens, 0.4s\n",
      "mistral: 98 tokens, 1.8s\n",
      "openai: 133 tokens, 1.0s\n",
      "llama: 161 tokens, 0.4s\n",
      "mistral: 130 tokens, 2.0s\n",
      "openai: 90 tokens, 1.2s\n",
      "llama: 118 tokens, 0.4s\n",
      "mistral: 88 tokens, 2.8s\n",
      "openai: 90 tokens, 0.9s\n",
      "llama: 118 tokens, 0.4s\n",
      "mistral: 88 tokens, 11.7s\n",
      "openai: 114 tokens, 1.2s\n",
      "llama: 142 tokens, 0.5s\n",
      "mistral: 112 tokens, 2.9s\n",
      "openai: 283 tokens, 1.1s\n",
      "llama: 312 tokens, 0.4s\n",
      "mistral: 285 tokens, 3.1s\n",
      "openai: 119 tokens, 0.6s\n",
      "llama: 176 tokens, 0.4s\n",
      "mistral: 153 tokens, 2.8s\n",
      "openai: 40 tokens, 0.7s\n",
      "llama: 94 tokens, 0.4s\n",
      "mistral: 61 tokens, 7.4s\n",
      "openai: 188 tokens, 1.1s\n",
      "llama: 217 tokens, 0.4s\n",
      "mistral: 192 tokens, 3.6s\n",
      "openai: 94 tokens, 1.6s\n",
      "llama: 124 tokens, 0.4s\n",
      "mistral: 97 tokens, 15.4s\n",
      "openai: 48 tokens, 0.7s\n",
      "llama: 95 tokens, 0.4s\n",
      "mistral: 63 tokens, 4.5s\n",
      "openai: 82 tokens, 1.0s\n",
      "llama: 124 tokens, 0.4s\n",
      "mistral: 88 tokens, 14.2s\n",
      "openai: 393 tokens, 1.6s\n",
      "llama: 423 tokens, 0.4s\n",
      "mistral: 397 tokens, 2.0s\n",
      "openai: 77 tokens, 1.4s\n",
      "llama: 105 tokens, 0.4s\n",
      "mistral: 73 tokens, 5.6s\n",
      "openai: 138 tokens, 1.0s\n",
      "llama: 167 tokens, 0.4s\n",
      "mistral: 134 tokens, 1.6s\n",
      "openai: 74 tokens, 1.0s\n",
      "llama: 103 tokens, 0.4s\n",
      "mistral: 71 tokens, 4.4s\n",
      "openai: 69 tokens, 1.1s\n",
      "llama: 97 tokens, 0.4s\n",
      "mistral: 65 tokens, 4.5s\n",
      "openai: 99 tokens, 0.9s\n",
      "llama: 128 tokens, 0.4s\n",
      "mistral: 102 tokens, 1.3s\n",
      "openai: 67 tokens, 1.1s\n",
      "llama: 112 tokens, 0.4s\n",
      "mistral: 67 tokens, 10.9s\n",
      "openai: 132 tokens, 0.9s\n",
      "llama: 172 tokens, 1.0s\n",
      "mistral: 139 tokens, 14.1s\n",
      "openai: 67 tokens, 1.0s\n",
      "llama: 96 tokens, 1.0s\n",
      "mistral: 64 tokens, 11.5s\n",
      "openai: 113 tokens, 0.3s\n",
      "llama: 142 tokens, 1.0s\n",
      "mistral: 112 tokens, 16.2s\n",
      "openai: 396 tokens, 1.3s\n",
      "llama: 428 tokens, 0.4s\n",
      "mistral: 401 tokens, 2.7s\n",
      "openai: 356 tokens, 1.2s\n",
      "llama: 388 tokens, 0.4s\n",
      "mistral: 366 tokens, 4.9s\n",
      "openai: 80 tokens, 1.0s\n",
      "llama: 108 tokens, 0.5s\n",
      "mistral: 76 tokens, 4.1s\n",
      "openai: 24 tokens, 0.3s\n",
      "llama: 49 tokens, 0.3s\n",
      "mistral: 60 tokens, 3.8s\n",
      "openai: 558 tokens, 0.9s\n",
      "llama: 576 tokens, 0.4s\n",
      "mistral: 621 tokens, 2.0s\n",
      "openai: 68 tokens, 0.9s\n",
      "llama: 97 tokens, 0.4s\n",
      "mistral: 65 tokens, 2.6s\n",
      "openai: 93 tokens, 0.9s\n",
      "llama: 117 tokens, 0.3s\n",
      "mistral: 111 tokens, 1.8s\n",
      "openai: 73 tokens, 1.1s\n",
      "llama: 103 tokens, 0.4s\n",
      "mistral: 71 tokens, 10.3s\n",
      "openai: 187 tokens, 1.0s\n",
      "llama: 215 tokens, 0.4s\n",
      "mistral: 186 tokens, 8.5s\n",
      "openai: 109 tokens, 1.0s\n",
      "llama: 138 tokens, 0.4s\n",
      "mistral: 108 tokens, 2.3s\n",
      "openai: 212 tokens, 1.1s\n",
      "llama: 283 tokens, 0.5s\n",
      "mistral: 255 tokens, 20.5s\n",
      "openai: 167 tokens, 0.4s\n",
      "llama: 241 tokens, 0.4s\n",
      "mistral: 213 tokens, 2.6s\n",
      "openai: 221 tokens, 1.1s\n",
      "llama: 276 tokens, 0.4s\n",
      "mistral: 232 tokens, 1.6s\n",
      "openai: 57 tokens, 1.2s\n",
      "llama: 94 tokens, 0.4s\n",
      "mistral: 62 tokens, 3.8s\n",
      "openai: 76 tokens, 1.2s\n",
      "llama: 104 tokens, 0.4s\n",
      "mistral: 74 tokens, 3.8s\n",
      "openai: 66 tokens, 1.1s\n",
      "llama: 94 tokens, 0.4s\n",
      "mistral: 62 tokens, 2.3s\n",
      "openai: 494 tokens, 0.5s\n",
      "llama: 622 tokens, 0.4s\n",
      "mistral: 535 tokens, 6.0s\n",
      "openai: 25 tokens, 0.4s\n",
      "llama: 61 tokens, 0.3s\n",
      "mistral: 62 tokens, 1.6s\n",
      "openai: 181 tokens, 0.3s\n",
      "llama: 259 tokens, 0.4s\n",
      "mistral: 235 tokens, 2.6s\n",
      "openai: 78 tokens, 0.9s\n",
      "llama: 106 tokens, 0.4s\n",
      "mistral: 74 tokens, 1.8s\n",
      "openai: 296 tokens, 1.2s\n",
      "llama: 324 tokens, 0.4s\n",
      "mistral: 296 tokens, 1.9s\n",
      "openai: 108 tokens, 1.0s\n",
      "llama: 105 tokens, 0.3s\n",
      "mistral: 107 tokens, 3.1s\n",
      "openai: 63 tokens, 1.4s\n",
      "llama: 91 tokens, 0.4s\n",
      "mistral: 59 tokens, 2.4s\n",
      "openai: 697 tokens, 1.0s\n",
      "llama: 715 tokens, 0.4s\n",
      "mistral: 724 tokens, 6.1s\n",
      "openai: 545 tokens, 1.3s\n",
      "llama: 575 tokens, 0.5s\n",
      "openai: 64 tokens, 0.9s\n",
      "llama: 92 tokens, 0.4s\n",
      "mistral: 60 tokens, 1.7s\n",
      "openai: 82 tokens, 1.9s\n",
      "llama: 110 tokens, 0.5s\n",
      "mistral: 81 tokens, 1.7s\n",
      "openai: 70 tokens, 1.2s\n",
      "llama: 99 tokens, 0.5s\n",
      "mistral: 68 tokens, 8.4s\n",
      "openai: 282 tokens, 1.0s\n",
      "llama: 318 tokens, 0.5s\n",
      "mistral: 298 tokens, 7.0s\n",
      "openai: 68 tokens, 0.9s\n",
      "llama: 98 tokens, 0.4s\n",
      "mistral: 65 tokens, 2.2s\n",
      "openai: 67 tokens, 1.3s\n",
      "llama: 96 tokens, 0.6s\n",
      "mistral: 65 tokens, 1.7s\n",
      "openai: 68 tokens, 1.0s\n",
      "llama: 65 tokens, 0.5s\n",
      "mistral: 64 tokens, 3.6s\n",
      "openai: 76 tokens, 1.0s\n",
      "llama: 120 tokens, 0.4s\n",
      "mistral: 90 tokens, 5.0s\n",
      "openai: 67 tokens, 1.0s\n",
      "llama: 95 tokens, 0.4s\n",
      "mistral: 63 tokens, 6.0s\n",
      "openai: 82 tokens, 0.8s\n",
      "llama: 111 tokens, 0.4s\n",
      "mistral: 79 tokens, 1.6s\n",
      "openai: 94 tokens, 1.0s\n",
      "llama: 124 tokens, 0.4s\n",
      "mistral: 93 tokens, 1.5s\n",
      "openai: 74 tokens, 1.2s\n",
      "llama: 102 tokens, 0.5s\n",
      "mistral: 70 tokens, 4.8s\n",
      "openai: 37 tokens, 0.5s\n",
      "llama: 65 tokens, 0.4s\n",
      "mistral: 33 tokens, 3.9s\n",
      "openai: 65 tokens, 0.9s\n",
      "llama: 93 tokens, 0.4s\n",
      "mistral: 61 tokens, 2.7s\n",
      "openai: 132 tokens, 1.0s\n",
      "llama: 161 tokens, 0.4s\n",
      "mistral: 128 tokens, 4.2s\n",
      "openai: 463 tokens, 0.2s\n",
      "llama: 476 tokens, 0.4s\n",
      "mistral: 485 tokens, 6.9s\n",
      "openai: 40 tokens, 0.6s\n",
      "llama: 103 tokens, 0.4s\n",
      "mistral: 72 tokens, 10.5s\n",
      "openai: 75 tokens, 1.3s\n",
      "llama: 103 tokens, 0.5s\n",
      "mistral: 72 tokens, 3.2s\n",
      "openai: 470 tokens, 1.1s\n",
      "llama: 568 tokens, 0.4s\n",
      "mistral: 468 tokens, 2.1s\n",
      "openai: 520 tokens, 0.8s\n",
      "llama: 571 tokens, 0.4s\n",
      "mistral: 535 tokens, 4.3s\n",
      "openai: 35 tokens, 0.4s\n",
      "llama: 80 tokens, 0.4s\n",
      "mistral: 50 tokens, 5.3s\n",
      "openai: 259 tokens, 3.2s\n",
      "llama: 321 tokens, 0.5s\n",
      "mistral: 260 tokens, 2.0s\n",
      "openai: 110 tokens, 1.4s\n",
      "llama: 137 tokens, 0.4s\n",
      "mistral: 107 tokens, 1.5s\n",
      "openai: 63 tokens, 1.5s\n",
      "llama: 91 tokens, 0.5s\n",
      "mistral: 59 tokens, 10.0s\n",
      "openai: 69 tokens, 1.2s\n",
      "llama: 100 tokens, 0.5s\n",
      "mistral: 69 tokens, 5.1s\n",
      "openai: 45 tokens, 0.6s\n",
      "llama: 103 tokens, 0.4s\n",
      "mistral: 70 tokens, 1.8s\n",
      "openai: 68 tokens, 1.0s\n",
      "llama: 97 tokens, 0.4s\n",
      "mistral: 63 tokens, 2.4s\n",
      "openai: 92 tokens, 1.1s\n",
      "llama: 122 tokens, 0.5s\n",
      "mistral: 96 tokens, 18.3s\n",
      "openai: 80 tokens, 0.9s\n",
      "llama: 109 tokens, 0.4s\n",
      "mistral: 77 tokens, 4.7s\n",
      "openai: 102 tokens, 1.0s\n",
      "llama: 135 tokens, 0.4s\n",
      "mistral: 101 tokens, 4.6s\n",
      "openai: 765 tokens, 1.0s\n",
      "llama: 793 tokens, 0.5s\n",
      "mistral: 912 tokens, 1.9s\n",
      "openai: 422 tokens, 0.3s\n",
      "llama: 462 tokens, 0.4s\n",
      "mistral: 446 tokens, 3.0s\n",
      "openai: 132 tokens, 1.0s\n",
      "llama: 160 tokens, 0.5s\n",
      "mistral: 129 tokens, 2.8s\n",
      "openai: 60 tokens, 1.0s\n",
      "llama: 95 tokens, 0.4s\n",
      "mistral: 63 tokens, 8.3s\n",
      "openai: 74 tokens, 1.3s\n",
      "llama: 102 tokens, 0.5s\n",
      "mistral: 70 tokens, 1.4s\n",
      "openai: 67 tokens, 1.4s\n",
      "llama: 94 tokens, 0.4s\n",
      "mistral: 64 tokens, 1.7s\n",
      "openai: 67 tokens, 1.7s\n",
      "llama: 96 tokens, 0.4s\n",
      "mistral: 64 tokens, 1.8s\n",
      "openai: 96 tokens, 0.2s\n",
      "llama: 124 tokens, 0.3s\n",
      "mistral: 94 tokens, 4.1s\n",
      "openai: 91 tokens, 1.1s\n",
      "llama: 120 tokens, 0.4s\n",
      "mistral: 89 tokens, 4.9s\n",
      "openai: 76 tokens, 1.0s\n",
      "llama: 104 tokens, 0.5s\n",
      "mistral: 72 tokens, 3.5s\n",
      "openai: 74 tokens, 1.1s\n",
      "llama: 102 tokens, 0.5s\n",
      "mistral: 72 tokens, 15.1s\n",
      "openai: 222 tokens, 0.3s\n",
      "llama: 303 tokens, 0.5s\n",
      "mistral: 273 tokens, 1.6s\n",
      "openai: 68 tokens, 0.8s\n",
      "llama: 97 tokens, 0.6s\n",
      "mistral: 66 tokens, 2.3s\n",
      "openai: 63 tokens, 0.8s\n",
      "llama: 81 tokens, 0.4s\n",
      "mistral: 59 tokens, 1.7s\n",
      "openai: 65 tokens, 1.1s\n",
      "llama: 93 tokens, 0.4s\n",
      "mistral: 61 tokens, 5.1s\n",
      "openai: 139 tokens, 0.6s\n",
      "llama: 195 tokens, 0.5s\n",
      "mistral: 187 tokens, 3.8s\n",
      "openai: 68 tokens, 1.3s\n",
      "llama: 97 tokens, 0.4s\n",
      "mistral: 64 tokens, 3.2s\n",
      "openai: 74 tokens, 0.8s\n",
      "llama: 102 tokens, 0.4s\n",
      "mistral: 70 tokens, 18.3s\n",
      "openai: 221 tokens, 1.2s\n",
      "llama: 225 tokens, 0.3s\n",
      "mistral: 229 tokens, 5.7s\n",
      "openai: 67 tokens, 1.1s\n",
      "llama: 99 tokens, 0.4s\n",
      "mistral: 63 tokens, 15.6s\n",
      "openai: 37 tokens, 0.5s\n",
      "llama: 64 tokens, 0.3s\n",
      "mistral: 34 tokens, 1.2s\n",
      "openai: 67 tokens, 1.0s\n",
      "llama: 95 tokens, 0.4s\n",
      "mistral: 63 tokens, 1.9s\n",
      "Completed: 299 successful calls, 47,877 total tokens\n"
     ]
    }
   ],
   "source": [
    "# Simplified prompt-centric data collection\n",
    "def run_performance_tests(max_prompts=100):\n",
    "    global data, results\n",
    "    \n",
    "    unprocessed = data[data['processed'] == 0].head(max_prompts)\n",
    "    if len(unprocessed) == 0:\n",
    "        print(\"All prompts processed\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing {len(unprocessed)} prompts with {len(models_to_test)} models\")\n",
    "    \n",
    "    # Simple prompt-centric structure with research metadata\n",
    "    experiment_id = f\"exp_{int(time.time())}\"\n",
    "    new_prompts = []\n",
    "    api_calls = {\"openai\": call_openai, \"llama\": call_llama, \"mistral\": call_mistral}\n",
    "    \n",
    "    for idx, row in unprocessed.iterrows():\n",
    "        prompt_text = row['prompt_text']\n",
    "        prompt_data = {\n",
    "            \"prompt_id\": f\"prompt_{idx}\",\n",
    "            \"prompt_text\": prompt_text,\n",
    "            \"experiment_id\": experiment_id,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"models\": {}\n",
    "        }\n",
    "        \n",
    "        for model_name in models_to_test:\n",
    "            try:\n",
    "                result = track_performance(model_name, prompt_text, api_calls[model_name])\n",
    "                prompt_data[\"models\"][model_name] = result\n",
    "                \n",
    "                if result['model_info']['success']:\n",
    "                    usage = result.get('usage_metrics', {})\n",
    "                    tokens = usage.get('total_tokens', 0)\n",
    "                    duration = result['performance']['duration_seconds']\n",
    "                    print(f\"{model_name}: {tokens} tokens, {duration:.1f}s\")\n",
    "                time.sleep(0.1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                prompt_data[\"models\"][model_name] = {\n",
    "                    \"model_info\": {\"name\": model_name, \"success\": False, \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")},\n",
    "                    \"error\": {\"message\": str(e), \"type\": type(e).__name__}\n",
    "                }\n",
    "                if \"quota\" in str(e).lower() or \"billing\" in str(e).lower():\n",
    "                    print(f\"CRITICAL: {model_name} - {str(e)[:50]}...\")\n",
    "                    return None\n",
    "        \n",
    "        new_prompts.append(prompt_data)\n",
    "        data.loc[idx, 'processed'] = 1\n",
    "    \n",
    "    # Save simplified data\n",
    "    data.to_json(\"data/sample.json\", orient='records', indent=2)\n",
    "    all_results = results + new_prompts\n",
    "    with open(\"data/energy.json\", \"w\") as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    \n",
    "    successful = sum(len([m for m in prompt[\"models\"].values() if m.get('model_info', {}).get('success', False)]) for prompt in new_prompts)\n",
    "    total_tokens = sum(m.get('usage_metrics', {}).get('total_tokens', 0) for prompt in new_prompts for m in prompt[\"models\"].values() if m.get('model_info', {}).get('success', False))\n",
    "    \n",
    "    print(f\"Completed: {successful} successful calls, {total_tokens:,} total tokens\")\n",
    "    return all_results\n",
    "\n",
    "# Reset all prompts to unprocessed (run this first if you want to start fresh)\n",
    "def reset_all_prompts():\n",
    "    global data\n",
    "    data['processed'] = 0\n",
    "    data.to_json(\"data/sample.json\", orient='records', indent=2)\n",
    "    print(\"All prompts reset to unprocessed\")\n",
    "\n",
    "# Uncomment the line below to reset all prompts to unprocessed\n",
    "# reset_all_prompts()\n",
    "\n",
    "# Run tests - change max_prompts to process all prompts\n",
    "if models_to_test:\n",
    "    # Change this number to process all prompts (e.g., max_prompts=100)\n",
    "    all_results = run_performance_tests(max_prompts=100)  # Process all available prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "698ea129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current status:\n",
      "  Total prompts: 100\n",
      "  Processed: 100\n",
      "  Unprocessed: 0\n",
      "  To process all: uncomment reset_all_prompts() above, then run the performance tests\n"
     ]
    }
   ],
   "source": [
    "# Check current status\n",
    "print(f\"Current status:\")\n",
    "print(f\"  Total prompts: {len(data)}\")\n",
    "print(f\"  Processed: {data['processed'].sum()}\")\n",
    "print(f\"  Unprocessed: {(data['processed'] == 0).sum()}\")\n",
    "print(f\"  To process all: uncomment reset_all_prompts() above, then run the performance tests\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "c769671c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RESEARCH ANALYSIS ===\n",
      "Total prompts: 100\n",
      "Successful calls: 299\n",
      "\n",
      "MODEL PERFORMANCE:\n",
      "  OPENAI: 100 calls, 14,744 tokens, 146.6 tokens/sec, 1.474400 kWh\n",
      "  LLAMA: 100 calls, 18,189 tokens, 412.8 tokens/sec, 0.909450 kWh\n",
      "  MISTRAL: 99 calls, 14,944 tokens, 29.1 tokens/sec, 1.195520 kWh\n",
      "\n",
      "TOTAL: 47,877 tokens, 3.579370 kWh across 100 prompts\n"
     ]
    }
   ],
   "source": [
    "# Simplified prompt-centric analysis\n",
    "if 'all_results' in locals() and all_results:\n",
    "    print(\"=== RESEARCH ANALYSIS ===\")\n",
    "    \n",
    "    # Extract all model results from prompts\n",
    "    all_model_results = []\n",
    "    for prompt in all_results:\n",
    "        for model_name, model_result in prompt.get(\"models\", {}).items():\n",
    "            if model_result.get('model_info', {}).get('success', False):\n",
    "                all_model_results.append(model_result)\n",
    "    \n",
    "    if all_model_results:\n",
    "        print(f\"Total prompts: {len(all_results)}\")\n",
    "        print(f\"Successful calls: {len(all_model_results)}\")\n",
    "        \n",
    "        # Group by model for analysis\n",
    "        model_stats = {}\n",
    "        for result in all_model_results:\n",
    "            model_name = result.get('model_info', {}).get('name', 'unknown')\n",
    "            if model_name not in model_stats:\n",
    "                model_stats[model_name] = {'total_tokens': 0, 'total_duration': 0, 'calls': 0}\n",
    "            \n",
    "            usage = result.get('usage_metrics', {})\n",
    "            if usage:\n",
    "                model_stats[model_name]['total_tokens'] += usage.get('total_tokens', 0)\n",
    "            model_stats[model_name]['total_duration'] += result.get('performance', {}).get('duration_seconds', 0)\n",
    "            model_stats[model_name]['calls'] += 1\n",
    "        \n",
    "        # Display results with energy metrics\n",
    "        print(\"\\nMODEL PERFORMANCE:\")\n",
    "        total_energy = 0\n",
    "        for model_name, stats in model_stats.items():\n",
    "            avg_speed = stats['total_tokens'] / stats['total_duration'] if stats['total_duration'] > 0 else 0\n",
    "            # Calculate energy from usage_metrics\n",
    "            model_energy = 0\n",
    "            for result in all_model_results:\n",
    "                if result.get('model_info', {}).get('name') == model_name:\n",
    "                    energy = result.get('usage_metrics', {}).get('estimated_energy_kwh', 0)\n",
    "                    model_energy += energy\n",
    "            total_energy += model_energy\n",
    "            print(f\"  {model_name.upper()}: {stats['calls']} calls, {stats['total_tokens']:,} tokens, {avg_speed:.1f} tokens/sec, {model_energy:.6f} kWh\")\n",
    "        \n",
    "        total_tokens = sum(stats['total_tokens'] for stats in model_stats.values())\n",
    "        print(f\"\\nTOTAL: {total_tokens:,} tokens, {total_energy:.6f} kWh across {len(all_results)} prompts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "7f22c7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENERGY.JSON STRUCTURE ===\n",
      "SAMPLE PROMPT:\n",
      "  - ID: prompt_0\n",
      "  - Text: how can identity protection services help protect me against identity theft...\n",
      "\n",
      "MODEL RESULTS:\n",
      "  OPENAI:\n",
      "    - Duration: 1.483s\n",
      "    - Tokens: 68\n",
      "    - Speed: 45.9 tokens/sec\n",
      "    - Response: Identity protection services can help safeguard you against identity theft in se...\n",
      "  LLAMA:\n",
      "    - Duration: 0.381s\n",
      "    - Tokens: 96\n",
      "    - Speed: 251.7 tokens/sec\n",
      "    - Response: Identity protection services can help protect you against identity theft in seve...\n",
      "  MISTRAL:\n",
      "    - Duration: 18.522s\n",
      "    - Tokens: 64\n",
      "    - Speed: 3.5 tokens/sec\n",
      "    - Response: Identity protection services can help safeguard you against identity theft by mo...\n",
      "\n",
      "DATA STRUCTURE:\n",
      "  - Total prompts: 100\n",
      "  - Each prompt contains: prompt_id, prompt_text, models\n",
      "  - Each model contains: model_info, performance, usage_metrics, response_data, raw_api_response\n"
     ]
    }
   ],
   "source": [
    "# Display simplified data structure\n",
    "if 'all_results' in locals() and all_results:\n",
    "    print(\"=== ENERGY.JSON STRUCTURE ===\")\n",
    "    \n",
    "    sample_prompt = all_results[0]\n",
    "    print(f\"SAMPLE PROMPT:\")\n",
    "    print(f\"  - ID: {sample_prompt['prompt_id']}\")\n",
    "    print(f\"  - Text: {sample_prompt['prompt_text'][:100]}...\")\n",
    "    \n",
    "    print(f\"\\nMODEL RESULTS:\")\n",
    "    for model_name, model_result in sample_prompt['models'].items():\n",
    "        if model_result.get('model_info', {}).get('success', False):\n",
    "            print(f\"  {model_name.upper()}:\")\n",
    "            print(f\"    - Duration: {model_result['performance']['duration_seconds']}s\")\n",
    "            usage = model_result.get('usage_metrics', {})\n",
    "            if usage:\n",
    "                print(f\"    - Tokens: {usage.get('total_tokens', 0)}\")\n",
    "                print(f\"    - Speed: {usage.get('tokens_per_second', 0):.1f} tokens/sec\")\n",
    "            response = model_result.get('response_data', {})\n",
    "            if response.get('content'):\n",
    "                print(f\"    - Response: {response['content'][:80]}...\")\n",
    "        else:\n",
    "            print(f\"  {model_name.upper()}: FAILED - {model_result.get('error', {}).get('message', 'Unknown error')}\")\n",
    "    \n",
    "    print(f\"\\nDATA STRUCTURE:\")\n",
    "    print(f\"  - Total prompts: {len(all_results)}\")\n",
    "    print(f\"  - Each prompt contains: prompt_id, prompt_text, models\")\n",
    "    print(f\"  - Each model contains: model_info, performance, usage_metrics, response_data, raw_api_response\")\n",
    "else:\n",
    "    print(\"No results available yet. Run the performance tests first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
