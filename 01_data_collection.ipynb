{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Collection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset, disable_caching\n",
        "from huggingface_hub import login\n",
        "\n",
        "try:\n",
        "    import orjson\n",
        "    def dumps(x): return orjson.dumps(x).decode()\n",
        "except Exception:\n",
        "    def dumps(x): return json.dumps(x, separators=(',', ':'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Hugging Face authentication token from environment or .env file\n",
        "def load_token():\n",
        "    t = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
        "    if t: return t\n",
        "    p = Path(\".env\")\n",
        "    if p.exists():\n",
        "        for line in p.read_text(encoding=\"utf-8\").splitlines():\n",
        "            if line.startswith(\"HUGGINGFACE_HUB_TOKEN=\") and not line.lstrip().startswith(\"#\"):\n",
        "                return line.split(\"=\", 1)[1].strip()\n",
        "    sys.stderr.write(\"Set HUGGINGFACE_HUB_TOKEN or add it to .env\\n\")\n",
        "    sys.exit(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display progress bar for data download tracking\n",
        "def progress(i, n):\n",
        "    if not n: return\n",
        "    k = 40\n",
        "    bar = \"=\" * int(k * i / n) + \"-\" * (k - int(k * i / n))\n",
        "    sys.stdout.write(f\"\\r[{bar}] {i:,}/{n:,} ({100*i/n:4.1f}%)\")\n",
        "    sys.stdout.flush()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stream and save dataset samples to JSONL file with progress tracking\n",
        "def stream(sample_size, out_path, split, step):\n",
        "    disable_caching()\n",
        "    login(token=load_token())\n",
        "    ds = load_dataset(\"lmsys/lmsys-chat-1m\", split=split, streaming=True)\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    wrote = 0\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\", buffering=1_048_576) as f:\n",
        "        for wrote, ex in enumerate(ds, 1):\n",
        "            f.write(dumps(ex) + \"\\n\")\n",
        "            if wrote % step == 0: progress(wrote, sample_size)\n",
        "            if wrote >= sample_size: break\n",
        "    progress(sample_size, sample_size)\n",
        "    return wrote\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure data collection parameters and execute download\n",
        "samples = 1_000_000\n",
        "output_path = Path(\"data/raw_conversations.jsonl\")\n",
        "split = \"train\"\n",
        "progress_every = 50_000\n",
        "\n",
        "n = stream(samples, output_path, split, progress_every)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
