{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5844de77",
   "metadata": {},
   "source": [
    "# Measuring Prompt Energy in LLMs\n",
    "\n",
    "Research tool for analyzing prompt characteristics vs. energy consumption across LLM models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8de833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dependencies and load environment\n",
    "import pandas as pd, openai, os, json, time, requests\n",
    "from dotenv import load_dotenv\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61e4f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prompts data and initialize processed status\n",
    "data = pd.read_json(\"data/prompts.jsonl\", lines=True)\n",
    "if 'processed' not in data.columns:\n",
    "    data['processed'] = 0\n",
    "    data.to_json(\"data/prompts.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a864d69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup API clients for different models\n",
    "load_dotenv(override=True)\n",
    "clients, models_to_test = {}, []\n",
    "\n",
    "if openai_key := os.getenv(\"OPENAI_API_KEY\"):\n",
    "    clients['openai'] = openai.OpenAI(api_key=openai_key)\n",
    "    models_to_test.append(\"openai\")\n",
    "\n",
    "if groq_key := os.getenv(\"GROQ_API_KEY\"):\n",
    "    clients['llama'] = groq_key\n",
    "    models_to_test.append(\"llama\")\n",
    "\n",
    "if mistral_key := os.getenv(\"MISTRAL_API_KEY\"):\n",
    "    clients['mistral'] = mistral_key\n",
    "    models_to_test.append(\"mistral\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0fe434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure energy consumption and performance metrics\n",
    "def track_performance(model_name, prompt, api_call_func):\n",
    "    tracker = EmissionsTracker(log_level=\"ERROR\", save_to_file=False)\n",
    "    tracker.start()\n",
    "    start_time = time.time()\n",
    "    response_data, time_to_first_token = api_call_func(prompt)\n",
    "    duration = time.time() - start_time\n",
    "    energy_consumed_wh = tracker.stop() * 1000\n",
    "    \n",
    "    usage = response_data.get(\"usage\", {})\n",
    "    prompt_tokens = usage.get(\"prompt_tokens\", usage.get(\"input_tokens\", 0))\n",
    "    completion_tokens = usage.get(\"completion_tokens\", usage.get(\"output_tokens\", 0))\n",
    "    total_tokens = usage.get(\"total_tokens\", prompt_tokens + completion_tokens)\n",
    "    content = response_data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"model\": response_data.get(\"model\", model_name),\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"duration\": round(duration, 2),\n",
    "        \"time_to_first_token\": round(time_to_first_token, 3),\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"completion_tokens\": completion_tokens,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"tokens_per_second\": round(total_tokens / duration, 1) if duration > 0 else 0,\n",
    "        \"energy_consumed_wh\": round(energy_consumed_wh, 4),\n",
    "        \"response\": content\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3850bd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API call functions for different models\n",
    "def call_openai(prompt):\n",
    "    start_time = time.time()\n",
    "    response = clients['openai'].chat.completions.create(\n",
    "        model=\"gpt-4o-mini\", messages=[{\"role\": \"user\", \"content\": prompt}], \n",
    "        max_tokens=50, temperature=0.3\n",
    "    )\n",
    "    return {\n",
    "        \"usage\": {\n",
    "            \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "            \"completion_tokens\": response.usage.completion_tokens,\n",
    "            \"total_tokens\": response.usage.total_tokens\n",
    "        },\n",
    "        \"choices\": [{\"message\": {\"content\": choice.message.content}} for choice in response.choices],\n",
    "        \"model\": response.model\n",
    "    }, time.time() - start_time\n",
    "\n",
    "def call_llama(prompt):\n",
    "    start_time = time.time()\n",
    "    response = requests.post(\n",
    "        \"https://api.groq.com/openai/v1/chat/completions\",\n",
    "        headers={\"Authorization\": f\"Bearer {clients['llama']}\"},\n",
    "        json={\"model\": \"llama-3.1-8b-instant\", \"messages\": [{\"role\": \"user\", \"content\": prompt}], \n",
    "              \"max_tokens\": 50, \"temperature\": 0.3}\n",
    "    )\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"API Error: {response.status_code}\")\n",
    "    return response.json(), time.time() - start_time\n",
    "\n",
    "def call_mistral(prompt):\n",
    "    start_time = time.time()\n",
    "    response = requests.post(\n",
    "        \"https://api.mistral.ai/v1/chat/completions\",\n",
    "        headers={\"Authorization\": f\"Bearer {clients['mistral']}\"},\n",
    "        json={\"model\": \"mistral-large-latest\", \"messages\": [{\"role\": \"user\", \"content\": prompt}], \n",
    "              \"max_tokens\": 50, \"temperature\": 0.3}\n",
    "    )\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"API Error: {response.status_code}\")\n",
    "    resp_json = response.json()\n",
    "    if \"error\" in resp_json:\n",
    "        raise Exception(f\"API Error: {resp_json['error']}\")\n",
    "    return resp_json, time.time() - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc272f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process prompts and save results incrementally\n",
    "def run_performance_tests(max_prompts=2):\n",
    "    global data\n",
    "    unprocessed = data[data['processed'] == 0].head(max_prompts)\n",
    "    if len(unprocessed) == 0:\n",
    "        return\n",
    "    \n",
    "    api_calls = {\"openai\": call_openai, \"llama\": call_llama, \"mistral\": call_mistral}\n",
    "    \n",
    "    for idx, row in unprocessed.iterrows():\n",
    "        prompt_text = row['prompt_text']\n",
    "        for model_name in models_to_test:\n",
    "            print(f\"Processing prompt {idx + 1} with {model_name}\")\n",
    "            result = track_performance(model_name, prompt_text, api_calls[model_name])\n",
    "            with open(\"data/energy.jsonl\", \"a\") as f:\n",
    "                f.write(json.dumps(result) + \"\\n\")\n",
    "        data.loc[idx, 'processed'] = 1\n",
    "        data.to_json(\"data/prompts.jsonl\", orient='records', lines=True)\n",
    "\n",
    "if models_to_test:\n",
    "    run_performance_tests(max_prompts=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd59f197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset processed status to reprocess all prompts\n",
    "data['processed'] = 0\n",
    "data.to_json(\"data/prompts.jsonl\", orient='records', lines=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
