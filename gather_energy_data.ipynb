{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5844de77",
   "metadata": {},
   "source": [
    "# Measuring Prompt Energy in LLMs\n",
    "\n",
    "**Research tool for analyzing prompt characteristics vs. energy consumption across LLM models**\n",
    "\n",
    "## Setup:\n",
    "1. Install: `pip install -r requirements.txt`\n",
    "2. Add API keys to `.env`:\n",
    "   ```\n",
    "   OPENAI_API_KEY=your_key\n",
    "   GROQ_API_KEY=your_key  \n",
    "   MISTRAL_API_KEY=your_key\n",
    "   ```\n",
    "3. Run cells to collect data\n",
    "\n",
    "## Measures:\n",
    "- Energy consumption (Wh) via CodeCarbon\n",
    "- Token usage and response timing\n",
    "- Time to first token\n",
    "- Performance across OpenAI, Groq, Mistral models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f8de833f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize dependencies\n",
    "import pandas as pd, openai, os, json, time, requests\n",
    "from dotenv import load_dotenv\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b61e4f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and results\n",
    "data = pd.read_json(\"data/sample.json\")\n",
    "if 'processed' not in data.columns:\n",
    "    data['processed'] = 0\n",
    "    data.to_json(\"data/sample.json\", orient='records', indent=2)\n",
    "\n",
    "try:\n",
    "    with open(\"data/energy.json\", \"r\") as f:\n",
    "        results = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a864d69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup API clients\n",
    "load_dotenv(override=True)\n",
    "clients, models_to_test = {}, []\n",
    "\n",
    "if openai_key := os.getenv(\"OPENAI_API_KEY\"):\n",
    "    clients['openai'] = openai.OpenAI(api_key=openai_key)\n",
    "    models_to_test.append(\"openai\")\n",
    "\n",
    "if groq_key := os.getenv(\"GROQ_API_KEY\"):\n",
    "    clients['llama'] = groq_key\n",
    "    models_to_test.append(\"llama\")\n",
    "\n",
    "if mistral_key := os.getenv(\"MISTRAL_API_KEY\"):\n",
    "    clients['mistral'] = mistral_key\n",
    "    models_to_test.append(\"mistral\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4d0fe434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track performance with energy measurement\n",
    "def track_performance(model_name, prompt, api_call_func):\n",
    "    tracker = EmissionsTracker(log_level=\"ERROR\", save_to_file=False)\n",
    "    tracker.start()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response_data, time_to_first_token = api_call_func(prompt)\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    energy_consumed_wh = tracker.stop() * 1000\n",
    "    \n",
    "    usage = response_data.get(\"usage\", {})\n",
    "    prompt_tokens = usage.get(\"prompt_tokens\", usage.get(\"input_tokens\", 0))\n",
    "    completion_tokens = usage.get(\"completion_tokens\", usage.get(\"output_tokens\", 0))\n",
    "    total_tokens = usage.get(\"total_tokens\", prompt_tokens + completion_tokens)\n",
    "    \n",
    "    content = response_data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"model\": response_data.get(\"model\", model_name),\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"duration\": round(duration, 2),\n",
    "        \"time_to_first_token\": round(time_to_first_token, 3),\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"completion_tokens\": completion_tokens,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"tokens_per_second\": round(total_tokens / duration, 1) if duration > 0 else 0,\n",
    "        \"energy_consumed_wh\": round(energy_consumed_wh, 4),\n",
    "        \"response\": content\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3850bd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API call functions\n",
    "def call_openai(prompt):\n",
    "    start_time = time.time()\n",
    "    response = clients['openai'].chat.completions.create(\n",
    "        model=\"gpt-4o-mini\", messages=[{\"role\": \"user\", \"content\": prompt}], \n",
    "        max_tokens=50, temperature=0.3\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"usage\": {\n",
    "            \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "            \"completion_tokens\": response.usage.completion_tokens,\n",
    "            \"total_tokens\": response.usage.total_tokens\n",
    "        },\n",
    "        \"choices\": [{\"message\": {\"content\": choice.message.content}} for choice in response.choices],\n",
    "        \"model\": response.model\n",
    "    }, time.time() - start_time\n",
    "\n",
    "def call_llama(prompt):\n",
    "    start_time = time.time()\n",
    "    response = requests.post(\n",
    "        \"https://api.groq.com/openai/v1/chat/completions\",\n",
    "        headers={\"Authorization\": f\"Bearer {clients['llama']}\"},\n",
    "        json={\"model\": \"llama-3.1-8b-instant\", \"messages\": [{\"role\": \"user\", \"content\": prompt}], \n",
    "              \"max_tokens\": 50, \"temperature\": 0.3}\n",
    "    )\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"API Error: {response.status_code}\")\n",
    "    \n",
    "    return response.json(), time.time() - start_time\n",
    "\n",
    "def call_mistral(prompt):\n",
    "    start_time = time.time()\n",
    "    response = requests.post(\n",
    "        \"https://api.mistral.ai/v1/chat/completions\",\n",
    "        headers={\"Authorization\": f\"Bearer {clients['mistral']}\"},\n",
    "        json={\"model\": \"mistral-large-latest\", \"messages\": [{\"role\": \"user\", \"content\": prompt}], \n",
    "              \"max_tokens\": 50, \"temperature\": 0.3}\n",
    "    )\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"API Error: {response.status_code}\")\n",
    "    resp_json = response.json()\n",
    "    if \"error\" in resp_json:\n",
    "        raise Exception(f\"API Error: {resp_json['error']}\")\n",
    "    \n",
    "    return resp_json, time.time() - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8fc272f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2 prompts with 3 models\n",
      "Completed: 6 calls, 558 total tokens\n"
     ]
    }
   ],
   "source": [
    "# Data collection with progress tracking\n",
    "def run_performance_tests(max_prompts=2):\n",
    "    global data, results\n",
    "    \n",
    "    unprocessed = data[data['processed'] == 0].head(max_prompts)\n",
    "    if len(unprocessed) == 0:\n",
    "        print(\"All prompts processed\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing {len(unprocessed)} prompts with {len(models_to_test)} models\")\n",
    "    \n",
    "    new_results = []\n",
    "    api_calls = {\"openai\": call_openai, \"llama\": call_llama, \"mistral\": call_mistral}\n",
    "    \n",
    "    for idx, row in unprocessed.iterrows():\n",
    "        prompt_text = row['prompt_text']\n",
    "        prompt_num = idx + 1\n",
    "        \n",
    "        for model_name in models_to_test:\n",
    "            print(f\"\\rProcessing prompt {prompt_num} with {model_name}...\", end=\"\", flush=True)\n",
    "            result = track_performance(model_name, prompt_text, api_calls[model_name])\n",
    "            new_results.append(result)\n",
    "        \n",
    "        data.loc[idx, 'processed'] = 1\n",
    "    \n",
    "    data.to_json(\"data/sample.json\", orient='records', indent=2)\n",
    "    all_results = results + new_results\n",
    "    with open(\"data/energy.json\", \"w\") as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    \n",
    "    total_tokens = sum(r['total_tokens'] for r in new_results)\n",
    "    print(f\"\\rCompleted: {len(new_results)} calls, {total_tokens:,} total tokens\")\n",
    "    return all_results\n",
    "\n",
    "if models_to_test:\n",
    "    all_results = run_performance_tests(max_prompts=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cd59f197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset processed status\n",
    "data['processed'] = 0\n",
    "data.to_json(\"data/sample.json\", orient='records', indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
